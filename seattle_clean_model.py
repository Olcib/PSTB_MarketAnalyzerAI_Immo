import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.impute import SimpleImputer
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

def load_seattle_clean():
    """
    Charge Seattle avec exclusion stricte des fuites
    """
    print("MOD√àLE SEATTLE SANS FUITES DE DONN√âES")
    print("="*45)
    
    df_seattle = pd.read_csv("data/processed/enriched_seattle_fixed.csv")
    
    # Variable cible
    if 'price' in df_seattle.columns:
        df_seattle['revenue'] = df_seattle['price']
    
    print(f"Dataset Seattle: {df_seattle.shape}")
    
    return df_seattle

def select_safe_features(df):
    """
    S√©lection stricte des features sans fuites
    """
    print("\nS√âLECTION DES FEATURES S√õRES")
    print("="*35)
    
    # Features INTERDITES (fuites potentielles)
    forbidden_patterns = [
        'price', 'revenue', 'cost', 'amount', 'dollar', 'euro', 
        'weekly_price', 'monthly_price', 'security_deposit', 
        'cleaning_fee', 'extra_people'
    ]
    
    # Features AUTORIS√âES (caract√©ristiques intrins√®ques)
    safe_base_features = [
        'nb_amenities', 'accommodates', 'bedrooms', 'beds', 'bathrooms',
        'latitude', 'longitude', 'month', 'day_of_week',
        'minimum_nights', 'maximum_nights', 'availability_365',
        'number_of_reviews', 'reviews_per_month'
    ]
    
    # Features de review scores (moyennes, pas individuelles)
    review_patterns = [
        'review_scores_rating', 'review_scores_accuracy', 
        'review_scores_cleanliness', 'review_scores_checkin',
        'review_scores_communication', 'review_scores_location',
        'review_scores_value'
    ]
    
    # Construire liste des features s√ªres
    safe_features = []
    
    # Features de base
    for feature in safe_base_features:
        if feature in df.columns:
            safe_features.append(feature)
    
    # Features de review (moyennes seulement, pas de lag/diff/rolling qui pourraient cr√©er des fuites)
    for pattern in review_patterns:
        if pattern in df.columns:
            safe_features.append(pattern)
    
    # V√©rifier qu'aucune feature interdite n'est incluse
    final_features = []
    for feature in safe_features:
        is_forbidden = any(forbidden in feature.lower() for forbidden in forbidden_patterns)
        if not is_forbidden:
            final_features.append(feature)
        else:
            print(f"  EXCLU (fuite potentielle): {feature}")
    
    print(f"\nFeatures s√ªres s√©lectionn√©es: {len(final_features)}")
    for i, feature in enumerate(final_features, 1):
        print(f"  {i:2d}. {feature}")
    
    return final_features

def prepare_clean_data(df, safe_features):
    """
    Pr√©paration des donn√©es sans fuites
    """
    print(f"\nPR√âPARATION DES DONN√âES PROPRES")
    print("="*40)
    
    # S√©lectionner features + target
    feature_cols = [f for f in safe_features if f in df.columns]
    
    if 'revenue' not in df.columns:
        print("Variable cible 'revenue' manquante")
        return None, None
    
    X = df[feature_cols].copy()
    y = df['revenue'].copy()
    
    print(f"Features finales: {len(feature_cols)}")
    print(f"√âchantillons: {len(X):,}")
    
    # Nettoyage des features
    for col in X.columns:
        if X[col].dtype == 'object':
            # Encoder les variables cat√©gorielles
            try:
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str).fillna('Unknown'))
                print(f"  Encod√©: {col}")
            except:
                X[col] = 0
        else:
            # Variables num√©riques
            X[col] = pd.to_numeric(X[col], errors='coerce')
    
    # V√©rifier les corr√©lations avec le prix (d√©tection finale)
    print(f"\nV√âRIFICATION FINALE DES CORR√âLATIONS:")
    suspicious_count = 0
    for col in X.columns:
        if X[col].notna().sum() > 100:
            try:
                corr = X[col].corr(y)
                if abs(corr) > 0.9:
                    print(f"  ALERTE: {col} corr√©lation = {corr:.3f}")
                    suspicious_count += 1
                elif abs(corr) > 0.7:
                    print(f"  √âlev√©e: {col} corr√©lation = {corr:.3f}")
            except:
                pass
    
    if suspicious_count == 0:
        print("  ‚úÖ Aucune corr√©lation suspecte d√©tect√©e")
    else:
        print(f"  ‚ö†Ô∏è {suspicious_count} corr√©lations suspectes restantes")
    
    # Imputation
    imputer = SimpleImputer(strategy='median')
    X_clean = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)
    
    # Normalisation
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(scaler.fit_transform(X_clean), columns=X_clean.columns)
    
    return X_scaled, y

def train_clean_seattle_model(X, y):
    """
    Entra√Ænement du mod√®le Seattle sans fuites
    """
    print(f"\nENTRA√éNEMENT MOD√àLE SEATTLE PROPRE")
    print("="*40)
    
    # Nettoyer les donn√©es
    mask = ~y.isnull()
    X_clean = X[mask]
    y_clean = y[mask]
    
    print(f"Donn√©es d'entra√Ænement: {len(X_clean):,} √©chantillons")
    
    # Split avec une proportion plus grande pour le test (donn√©es limit√©es)
    X_train, X_test, y_train, y_test = train_test_split(
        X_clean, y_clean, test_size=0.3, random_state=42
    )
    
    print(f"Train: {len(X_train):,} | Test: {len(X_test):,}")
    
    # Mod√®le conservateur pour √©viter l'overfitting
    model = lgb.LGBMRegressor(
        n_estimators=100,      # Peu d'arbres
        max_depth=5,           # Profondeur limit√©e
        learning_rate=0.1,     # Learning rate standard
        num_leaves=20,         # Peu de feuilles
        min_child_samples=20,  # Minimum d'√©chantillons par feuille
        subsample=0.8,         # Subsampling pour r√©gularisation
        random_state=42,
        verbose=-1
    )
    
    # Entra√Ænement
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    # M√©triques
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    
    # Validation crois√©e
    from sklearn.model_selection import cross_val_score
    cv_scores = cross_val_score(model, X_train, y_train, cv=3, scoring='r2')  # 3-fold seulement
    
    print(f"\nPERFORMANCE MOD√àLE PROPRE:")
    print(f"  RMSE: ${rmse:.2f}")
    print(f"  R¬≤: {r2:.3f}")
    print(f"  CV R¬≤: {cv_scores.mean():.3f}¬±{cv_scores.std():.3f}")
    
    # Comparaison avec mod√®le contamin√©
    print(f"\nCOMPARAISON:")
    print(f"  Mod√®le contamin√© (Jour 6): R¬≤ = 0.974")
    print(f"  Mod√®le propre (Actuel):    R¬≤ = {r2:.3f}")
    
    performance_drop = ((0.974 - r2) / 0.974) * 100
    print(f"  Chute de performance: -{performance_drop:.1f}%")
    
    if performance_drop > 70:
        print("  ‚úÖ CONFIRMATION: Fuites de donn√©es √©limin√©es")
    else:
        print("  ‚ö†Ô∏è Performance encore √©lev√©e - investigation suppl√©mentaire requise")
    
    # Feature importance
    print(f"\nIMPORTANCE DES FEATURES (Top 10):")
    feature_importance = pd.DataFrame({
        'feature': X.columns,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    for i, (_, row) in enumerate(feature_importance.head(10).iterrows(), 1):
        print(f"  {i:2d}. {row['feature']}: {row['importance']:.3f}")
    
    return model, r2, rmse

def main():
    """
    Reconstruction compl√®te du mod√®le Seattle
    """
    print("üîß RECONSTRUCTION MOD√àLE SEATTLE SANS FUITES")
    print("="*60)
    
    try:
        # Charger donn√©es
        df_seattle = load_seattle_clean()
        
        # S√©lectionner features s√ªres
        safe_features = select_safe_features(df_seattle)
        
        if len(safe_features) < 5:
            print("Pas assez de features s√ªres disponibles")
            return None
        
        # Pr√©parer donn√©es
        X, y = prepare_clean_data(df_seattle, safe_features)
        
        if X is None:
            print("√âchec de la pr√©paration des donn√©es")
            return None
        
        # Entra√Æner mod√®le propre
        model, r2, rmse = train_clean_seattle_model(X, y)
        
        print(f"\nüèÅ RECONSTRUCTION TERMIN√âE")
        print("="*35)
        
        if r2 > 0.3:
            print(f"‚úÖ Performance l√©gitime: R¬≤ = {r2:.3f}")
            print("   Mod√®le utilisable en production")
        elif r2 > 0.1:
            print(f"üî∂ Performance mod√©r√©e: R¬≤ = {r2:.3f}")
            print("   Am√©lioration possible avec plus de features")
        else:
            print(f"‚ùå Performance faible: R¬≤ = {r2:.3f}")
            print("   Donn√©es insuffisantes pour Seattle")
        
        return model, r2, rmse
        
    except Exception as e:
        print(f"\n‚ùå Erreur durant la reconstruction: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    result = main()