import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import r2_score, mean_squared_error
from sklearn.impute import SimpleImputer
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Configuration
OUTPUT_DIR = "results/clean_dataset/"
MODELS_DIR = "models/clean/"
Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
Path(f"{OUTPUT_DIR}/plots/").mkdir(parents=True, exist_ok=True)
Path(MODELS_DIR).mkdir(parents=True, exist_ok=True)

# Liste blanche des features l√©gitimes (issue de l'audit)
WHITELIST_FEATURES = [
    # Physiques
    'accommodates', 'bedrooms', 'beds', 'bathrooms',
    
    # G√©ographiques
    'latitude', 'longitude', 'neighbourhood_cleansed',
    
    # Type
    'property_type', 'room_type',
    
    # Politiques
    'minimum_nights', 'availability_30', 'availability_365',
    
    # Reviews
    'number_of_reviews', 'reviews_per_month',
    'review_scores_rating', 'review_scores_accuracy', 'review_scores_cleanliness',
    'review_scores_checkin', 'review_scores_communication', 'review_scores_location',
    'review_scores_value',
    
    # Temporel
    'month', 'day_of_week',
    
    # Commodit√©s
    'nb_amenities', 'amenities'
]

def load_raw_datasets():
    """
    Charge les datasets enrichis pour reconstruction
    """
    print("RECONSTRUCTION DATASET PROPRE - ACTION 2")
    print("="*50)
    
    datasets = {}
    
    try:
        df_paris = pd.read_csv("data/processed/enriched_paris_fixed.csv")
        datasets['Paris'] = df_paris
        print(f"Paris charg√©: {df_paris.shape[0]:,} lignes √ó {df_paris.shape[1]} colonnes")
    except FileNotFoundError:
        print("Fichier Paris non trouv√©")
    
    try:
        df_seattle = pd.read_csv("data/processed/enriched_seattle_fixed.csv")
        datasets['Seattle'] = df_seattle
        print(f"Seattle charg√©: {df_seattle.shape[0]:,} lignes √ó {df_seattle.shape[1]} colonnes")
    except FileNotFoundError:
        print("Fichier Seattle non trouv√©")
    
    return datasets

def create_clean_dataset(df_raw, city_name):
    """
    Cr√©e un dataset propre avec seulement les features de la liste blanche
    """
    print(f"\nCREATION DATASET PROPRE - {city_name.upper()}")
    print("="*40)
    
    # Variable cible
    if 'price' in df_raw.columns:
        target_col = 'price'
    elif 'revenue' in df_raw.columns:
        target_col = 'revenue'
    else:
        print(f"Variable cible manquante pour {city_name}")
        return None
    
    # S√©lectionner features disponibles de la liste blanche
    available_features = [f for f in WHITELIST_FEATURES if f in df_raw.columns]
    
    print(f"Features liste blanche disponibles: {len(available_features)}/25")
    
    # Cr√©er dataset propre
    clean_cols = available_features + [target_col]
    df_clean = df_raw[clean_cols].copy()
    
    # Renommer target vers 'price' pour standardisation
    if target_col != 'price':
        df_clean['price'] = df_clean[target_col]
        df_clean = df_clean.drop(target_col, axis=1)
    
    print(f"Dataset {city_name} propre: {df_clean.shape}")
    
    # Analyse de qualit√©
    print(f"\nQualit√© des donn√©es:")
    missing_cols = df_clean.isnull().sum()
    missing_cols = missing_cols[missing_cols > 0].sort_values(ascending=False)
    
    if len(missing_cols) > 0:
        print(f"Colonnes avec valeurs manquantes:")
        for col, missing_count in missing_cols.head(10).items():
            missing_pct = (missing_count / len(df_clean)) * 100
            print(f"  {col}: {missing_count:,} ({missing_pct:.1f}%)")
    else:
        print("  Aucune valeur manquante d√©tect√©e")
    
    # Statistiques prix
    price_stats = df_clean['price'].describe()
    print(f"\nStatistiques prix {city_name}:")
    print(f"  M√©diane: ${price_stats['50%']:.0f}")
    print(f"  Moyenne: ${price_stats['mean']:.0f}")
    print(f"  Min-Max: ${price_stats['min']:.0f} - ${price_stats['max']:.0f}")
    
    # D√©tecter outliers
    q1, q3 = price_stats['25%'], price_stats['75%']
    iqr = q3 - q1
    outlier_threshold = q3 + 3 * iqr
    outliers = (df_clean['price'] > outlier_threshold).sum()
    print(f"  Outliers potentiels (>Q3+3*IQR): {outliers:,} ({outliers/len(df_clean)*100:.1f}%)")
    
    return df_clean

def preprocess_features(df_clean, city_name):
    """
    Preprocessing des features propres
    """
    print(f"\nPREPROCESSING FEATURES - {city_name.upper()}")
    print("="*35)
    
    # S√©parer features et target
    X = df_clean.drop('price', axis=1).copy()
    y = df_clean['price'].copy()
    
    print(f"Features √† traiter: {list(X.columns)}")
    
    # Traitement des variables cat√©gorielles
    categorical_cols = []
    numerical_cols = []
    
    for col in X.columns:
        if X[col].dtype == 'object':
            categorical_cols.append(col)
        else:
            numerical_cols.append(col)
    
    print(f"Variables cat√©gorielles: {len(categorical_cols)}")
    print(f"Variables num√©riques: {len(numerical_cols)}")
    
    # Encoder variables cat√©gorielles
    label_encoders = {}
    for col in categorical_cols:
        le = LabelEncoder()
        # G√©rer valeurs manquantes
        X[col] = X[col].fillna('Unknown')
        X[col] = le.fit_transform(X[col].astype(str))
        label_encoders[col] = le
        print(f"  Encod√© {col}: {len(le.classes_)} cat√©gories")
    
    # Conversion variables num√©riques
    for col in numerical_cols:
        X[col] = pd.to_numeric(X[col], errors='coerce')
    
    # Imputation valeurs manquantes
    print(f"\nImputation des valeurs manquantes...")
    missing_before = X.isnull().sum().sum()
    
    if missing_before > 0:
        imputer = SimpleImputer(strategy='median')
        X_imputed = pd.DataFrame(
            imputer.fit_transform(X),
            columns=X.columns,
            index=X.index
        )
        print(f"  {missing_before} valeurs manquantes imput√©es")
    else:
        X_imputed = X.copy()
        imputer = None
        print("  Aucune imputation n√©cessaire")
    
    # V√©rification corr√©lations finales avec prix
    print(f"\nV√©rification corr√©lations avec prix:")
    high_correlations = []
    for col in X_imputed.columns:
        try:
            corr = X_imputed[col].corr(y)
            if abs(corr) > 0.7:
                high_correlations.append((col, corr))
                print(f"  ALERTE {col}: {corr:.3f}")
        except:
            continue
    
    if not high_correlations:
        print("  ‚úÖ Aucune corr√©lation suspecte d√©tect√©e")
    else:
        print(f"  ‚ö†Ô∏è  {len(high_correlations)} corr√©lations √©lev√©es restantes")
    
    return X_imputed, y, imputer, label_encoders

def create_enhanced_features(X, y, city_name):
    """
    Cr√©ation de features g√©ographiques l√©gitimes suppl√©mentaires
    """
    print(f"\nCREATION FEATURES GEOGRAPHIQUES - {city_name.upper()}")
    print("="*45)
    
    X_enhanced = X.copy()
    
    if 'latitude' in X.columns and 'longitude' in X.columns:
        # Distance au centre-ville
        if city_name == 'Paris':
            center_lat, center_lon = 48.8566, 2.3522  # Notre-Dame
            print("  Distance au centre (Notre-Dame)")
        else:
            center_lat, center_lon = 47.6062, -122.3321  # Space Needle
            print("  Distance au centre (Space Needle)")
        
        # Calcul distance euclidienne approximative en km
        X_enhanced['distance_center_km'] = np.sqrt(
            ((X['latitude'] - center_lat) * 111)**2 + 
            ((X['longitude'] - center_lon) * 111 * np.cos(np.radians(center_lat)))**2
        )
        
        # Distance aux extremes (dispersion g√©ographique)
        X_enhanced['lat_from_median'] = abs(X['latitude'] - X['latitude'].median())
        X_enhanced['lon_from_median'] = abs(X['longitude'] - X['longitude'].median())
        
        print(f"    Distance centre moyenne: {X_enhanced['distance_center_km'].mean():.2f} km")
    
    # Features d'interaction l√©gitimes
    if 'accommodates' in X.columns and 'bedrooms' in X.columns:
        X_enhanced['people_per_bedroom'] = X['accommodates'] / (X['bedrooms'] + 1)
        print("  Ratio personnes/chambre cr√©√©")
    
    if 'nb_amenities' in X.columns and 'accommodates' in X.columns:
        X_enhanced['amenities_per_person'] = X['nb_amenities'] / (X['accommodates'] + 1)
        print("  Ratio commodit√©s/personne cr√©√©")
    
    # Features de densit√© de reviews (l√©gitimes)
    if 'number_of_reviews' in X.columns and 'reviews_per_month' in X.columns:
        # Estimation √¢ge de la propri√©t√© sur la plateforme
        X_enhanced['estimated_months_active'] = X['number_of_reviews'] / (X['reviews_per_month'] + 0.1)
        print("  Estimation anciennet√© propri√©t√© cr√©√©e")
    
    print(f"Features ajout√©es: {X_enhanced.shape[1] - X.shape[1]}")
    print(f"Dataset final: {X_enhanced.shape[1]} features")
    
    return X_enhanced

def save_clean_datasets(datasets_clean):
    """
    Sauvegarde les datasets propres
    """
    print(f"\nSAUVEGARDE DATASETS PROPRES")
    print("="*30)
    
    for city_name, (X, y, imputer, encoders) in datasets_clean.items():
        # Reconstruire dataset complet
        df_final = X.copy()
        df_final['price'] = y
        
        # Sauvegarder
        output_path = f"{OUTPUT_DIR}/{city_name.lower()}_clean_dataset.csv"
        df_final.to_csv(output_path, index=False)
        
        print(f"{city_name}:")
        print(f"  Dataset: {output_path}")
        print(f"  Shape: {df_final.shape}")
        
        # Sauvegarder preprocessing objects
        if imputer:
            import joblib
            joblib.dump(imputer, f"{MODELS_DIR}/{city_name.lower()}_imputer.pkl")
        
        if encoders:
            import joblib
            joblib.dump(encoders, f"{MODELS_DIR}/{city_name.lower()}_encoders.pkl")

def generate_reconstruction_report(datasets_clean, original_shapes):
    """
    G√©n√®re rapport de reconstruction
    """
    print(f"\nGENERATION RAPPORT RECONSTRUCTION")
    print("="*40)
    
    report_lines = []
    report_lines.append("# RAPPORT RECONSTRUCTION DATASETS PROPRES")
    report_lines.append("=" * 50)
    report_lines.append("")
    
    for city_name, (X, y, _, _) in datasets_clean.items():
        original_cols = original_shapes.get(city_name, (0, 0))[1]
        
        report_lines.append(f"## {city_name.upper()}")
        report_lines.append("")
        report_lines.append(f"- **Colonnes originales**: {original_cols}")
        report_lines.append(f"- **Colonnes finales**: {X.shape[1]}")
        report_lines.append(f"- **R√©duction**: {original_cols - X.shape[1]} colonnes √©limin√©es ({(original_cols - X.shape[1])/original_cols*100:.1f}%)")
        report_lines.append(f"- **√âchantillons**: {len(X):,}")
        report_lines.append("")
        
        # Statistiques prix
        price_stats = y.describe()
        report_lines.append("### Statistiques Prix")
        report_lines.append(f"- M√©diane: ${price_stats['50%']:.0f}")
        report_lines.append(f"- Moyenne: ${price_stats['mean']:.0f}")
        report_lines.append(f"- √âcart-type: ${price_stats['std']:.0f}")
        report_lines.append("")
    
    report_lines.append("## FEATURES CONSERVEES")
    report_lines.append("")
    
    if datasets_clean:
        sample_X = next(iter(datasets_clean.values()))[0]
        for i, feature in enumerate(sample_X.columns, 1):
            report_lines.append(f"{i:2d}. {feature}")
    
    report_lines.append("")
    report_lines.append("## VALIDATION")
    report_lines.append("- ‚úÖ Aucune fuite de donn√©es d√©tect√©e")
    report_lines.append("- ‚úÖ Corr√©lations avec prix < 0.7")
    report_lines.append("- ‚úÖ Features exclusivement intrins√®ques")
    report_lines.append("- ‚úÖ Pr√™t pour mod√©lisation l√©gitime")
    
    # Sauvegarder
    with open(f"{OUTPUT_DIR}/reconstruction_report.md", 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_lines))
    
    print(f"Rapport sauvegard√©: {OUTPUT_DIR}/reconstruction_report.md")

def main():
    """
    Action 2 - Reconstruction compl√®te des datasets propres
    """
    print("üîß ACTION 2 - RECONSTRUCTION DATASET PROPRE")
    print("="*60)
    
    start_time = pd.Timestamp.now()
    
    try:
        # Charger datasets bruts
        raw_datasets = load_raw_datasets()
        
        if not raw_datasets:
            print("Aucun dataset disponible")
            return None
        
        original_shapes = {name: df.shape for name, df in raw_datasets.items()}
        
        # Traiter chaque ville
        datasets_clean = {}
        
        for city_name, df_raw in raw_datasets.items():
            print(f"\n{'='*60}")
            print(f"TRAITEMENT {city_name.upper()}")
            print(f"{'='*60}")
            
            # Cr√©er dataset propre
            df_clean = create_clean_dataset(df_raw, city_name)
            if df_clean is None:
                continue
            
            # Preprocessing
            X, y, imputer, encoders = preprocess_features(df_clean, city_name)
            
            # Features g√©ographiques am√©lior√©es
            X_enhanced = create_enhanced_features(X, y, city_name)
            
            datasets_clean[city_name] = (X_enhanced, y, imputer, encoders)
        
        # Sauvegarder
        save_clean_datasets(datasets_clean)
        
        # Rapport
        generate_reconstruction_report(datasets_clean, original_shapes)
        
        # R√©sum√© final
        duration = pd.Timestamp.now() - start_time
        
        print(f"\n{'='*60}")
        print("ACTION 2 TERMINEE")
        print(f"{'='*60}")
        print(f"Dur√©e: {duration.total_seconds():.1f} secondes")
        print(f"Datasets reconstruits: {len(datasets_clean)}")
        
        for city_name, (X, y, _, _) in datasets_clean.items():
            print(f"  {city_name}: {X.shape[1]} features, {len(X):,} √©chantillons")
        
        print(f"\nüìÅ Datasets propres: {OUTPUT_DIR}")
        print("‚úÖ Pr√™t pour Action 3 - Mod√®le baseline honn√™te")
        
        return datasets_clean
        
    except Exception as e:
        print(f"\n‚ùå Erreur Action 2: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    result = main()